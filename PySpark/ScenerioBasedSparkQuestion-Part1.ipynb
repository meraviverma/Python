{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    " .master(\"local\") \\\n",
    "     .appName(\"TrainingDay1\")\\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF Capitalize the letter of the Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[1,\"john jones\"],[2,\"tracey smith\"],[3,\"amy sanders\"]]\n",
    "columns=['seqno','names']\n",
    "df=spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toUpper(name):\n",
    "    return name.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "toupper = udf(lambda z: toUpper(z),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn(\"upper\",toupper(col('names')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|seqno|       names|\n",
      "+-----+------------+\n",
      "|    1|  john jones|\n",
      "|    2|tracey smith|\n",
      "|    3| amy sanders|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seqno', 'names']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=df.select('names').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john jones', 'tracey smith', 'amy sanders']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another approach is using annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def say_hello(name):\n",
    "     return f\"Hello {name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn(\"greet\",say_hello(col('names')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+------------------+\n",
      "|seqno|       names|       upper|             greet|\n",
      "+-----+------------+------------+------------------+\n",
      "|    1|  john jones|  JOHN JONES|  Hello john jones|\n",
      "|    2|tracey smith|TRACEY SMITH|Hello tracey smith|\n",
      "|    3| amy sanders| AMY SANDERS| Hello amy sanders|\n",
      "+-----+------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register function is also an approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.toUpper(name)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"toupper\", toUpper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|toupper(names)|\n",
      "+--------------+\n",
      "|    JOHN JONES|\n",
      "|  TRACEY SMITH|\n",
      "|   AMY SANDERS|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select toupper(names) from df_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get value of a particular cell in PySpark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =[[\"1\",\"sravan\",\"company 1\"],\n",
    "       [\"2\",\"ojaswi\",\"company 2\"],\n",
    "       [\"3\",\"bobby\",\"company 3\"],\n",
    "       [\"4\",\"rohith\",\"company 2\"],\n",
    "       [\"5\",\"gnanesh\",\"company 1\"]]\n",
    "  \n",
    "# specify column names\n",
    "columns=['Employee ID','Employee NAME',\n",
    "         'Company Name']\n",
    "  \n",
    "# creating a dataframe from the lists of data\n",
    "dataframe = spark.createDataFrame(data,columns)\n",
    "  \n",
    "# display dataframe\n",
    "dataframe.select(\"Employee ID\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(var):\n",
    "    print(dataframe.collect()[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first row= 1\n"
     ]
    }
   ],
   "source": [
    "print(\"first row=\",dataframe.collect()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to TRIM leading spaces in values of column in Pyspark in Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullDF=spark.createDataFrame([\n",
    "    (11,\"  Ravi\",100),(12,\"   Verma\", 200),(14, \"       Sam\",300)\n",
    "],[\"id\",\"name\",\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, ltrim,rtrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthDF=nullDF.withColumn(\"length\",length(nullDF.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Length here are 6 8 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+------+\n",
      "| id|      name|salary|length|\n",
      "+---+----------+------+------+\n",
      "| 11|      Ravi|   100|     6|\n",
      "| 12|     Verma|   200|     8|\n",
      "| 14|       Sam|   300|    10|\n",
      "+---+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lengthDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df=lengthDF.withColumn(\"name\",ltrim(nullDF.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df=trimmed_df.withColumn(\"length\",length(trimmed_df.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Length has been reduced to 4 5 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| id| name|salary|length|\n",
      "+---+-----+------+------+\n",
      "| 11| Ravi|   100|     4|\n",
      "| 12|Verma|   200|     5|\n",
      "| 14|  Sam|   300|     3|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trimmed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to TRIM trailing spaces in values of column in Pyspark\n",
    "## How to TRIM leading & trailing spaces in values of column in Pyspark in Databricks ?\n",
    "#### Using trim we can trim both leading and trailing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullDF=spark.createDataFrame([\n",
    "    (11,\"Ravi     \",100),(12,\"Verma     \", 200),(14, \"Sammy\",300)\n",
    "],[\"id\",\"name\",\"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthDF=nullDF.withColumn(\"length\",length(nullDF.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+------+\n",
      "| id|      name|salary|length|\n",
      "+---+----------+------+------+\n",
      "| 11| Ravi     |   100|     9|\n",
      "| 12|Verma     |   200|    10|\n",
      "| 14|     Sammy|   300|     5|\n",
      "+---+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lengthDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df=lengthDF.withColumn(\"name\",rtrim(nullDF.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_df=trimmed_df.withColumn(\"length\",length(trimmed_df.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| id| name|salary|length|\n",
      "+---+-----+------+------+\n",
      "| 11| Ravi|   100|     4|\n",
      "| 12|Verma|   200|     5|\n",
      "| 14|Sammy|   300|     5|\n",
      "+---+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trimmed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Remove the space in Columns & converting into camelCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[[\"1\",\"sravan\",100],\n",
    "       [\"2\",\"ojaswi\",None],\n",
    "       [\"3\",\"bobby\",102],\n",
    "       [\"4\",\"rohith\",None],\n",
    "       [\"5\",\"gnanesh\",200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify column names\n",
    "columns=['emp id','first name',\n",
    "         'emp salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf=spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+\n",
      "|emp id|first name|emp salary|\n",
      "+------+----------+----------+\n",
      "|     1|    sravan|       100|\n",
      "|     2|    ojaswi|      null|\n",
      "|     3|     bobby|       102|\n",
      "|     4|    rohith|      null|\n",
      "|     5|   gnanesh|       200|\n",
      "+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mydf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp id: string (nullable = true)\n",
      " |-- first name: string (nullable = true)\n",
      " |-- emp salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mydf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=mydf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleCols=[''.join(j for j in i.title() if not j.isspace()) for i in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "camelCols=[column[0].lower()+column[1:] for column in titleCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalColsDf=mydf.toDF(*camelCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+\n",
      "|empId|firstName|empSalary|\n",
      "+-----+---------+---------+\n",
      "|    1|   sravan|      100|\n",
      "|    2|   ojaswi|     null|\n",
      "|    3|    bobby|      102|\n",
      "|    4|   rohith|     null|\n",
      "|    5|  gnanesh|      200|\n",
      "+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalColsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empId: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- empSalary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalColsDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [emp id#221 AS empId#237, first name#222 AS firstName#238, emp salary#223L AS empSalary#239L]\n",
      "+- LogicalRDD [emp id#221, first name#222, emp salary#223L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "empId: string, firstName: string, empSalary: bigint\n",
      "Project [emp id#221 AS empId#237, first name#222 AS firstName#238, emp salary#223L AS empSalary#239L]\n",
      "+- LogicalRDD [emp id#221, first name#222, emp salary#223L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [emp id#221 AS empId#237, first name#222 AS firstName#238, emp salary#223L AS empSalary#239L]\n",
      "+- LogicalRDD [emp id#221, first name#222, emp salary#223L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [emp id#221 AS empId#237, first name#222 AS firstName#238, emp salary#223L AS empSalary#239L]\n",
      "+- Scan ExistingRDD[emp id#221,first name#222,emp salary#223L]\n"
     ]
    }
   ],
   "source": [
    "finalColsDf.explain(extended = \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get number of partition of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =[[\"1\",\"sravan\",\"company 1\"],\n",
    "       [\"2\",\"ojaswi\",\"company 2\"],\n",
    "       [\"3\",\"bobby\",\"company 3\"],\n",
    "       [\"4\",\"rohith\",\"company 2\"],\n",
    "       [\"5\",\"gnanesh\",\"company 1\"]]\n",
    "  \n",
    "# specify column names\n",
    "columns=['Employee ID','Employee NAME',\n",
    "         'Company Name']\n",
    "  \n",
    "# creating a dataframe from the lists of data\n",
    "my_dataframe = spark.createDataFrame(data_df,columns)\n",
    "  \n",
    "# display dataframe\n",
    "#dataframe.select(\"Employee ID\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataframe=my_dataframe.withColumn(\"partition_id\",spark_partition_id()).groupBy(\"partition_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0|    5|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+\n",
      "|Employee ID|Employee NAME|Company Name|\n",
      "+-----------+-------------+------------+\n",
      "|          1|       sravan|   company 1|\n",
      "|          2|       ojaswi|   company 2|\n",
      "|          3|        bobby|   company 3|\n",
      "|          4|       rohith|   company 2|\n",
      "|          5|      gnanesh|   company 1|\n",
      "+-----------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given a record calculate the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Input\n",
    "* year    runs \n",
    "* 2016    360\n",
    "* 2017    312\n",
    "* 2018    275\n",
    "* 2019    301\n",
    "* 2020    103\n",
    "\n",
    " \n",
    "\n",
    " output:\n",
    "* year    runs    performance\n",
    "* 2016    360        null\n",
    "* 2017    312        bad\n",
    "* 2018    275        bad\n",
    "* 2019    301        good\n",
    "* 2020    103        bad\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[2016,360],[2017,312],[2018,275],[2019,301],[2020,103]]\n",
    "columns=['year','runs']\n",
    "score_df=spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|year|runs|\n",
      "+----+----+\n",
      "|2016| 360|\n",
      "|2017| 312|\n",
      "|2018| 275|\n",
      "|2019| 301|\n",
      "|2020| 103|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df=score_df.withColumn(\"perf\",lag('runs').over(Window.orderBy(asc('year'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df=score_df.withColumn(\"out\",when( col(\"runs\") < col(\"perf\"),'bad').otherwise('good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df=score_df.withColumn(\"out\",when(score_df.perf.isNull(),'null').otherwise(col('out')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|year|runs|perf| out|\n",
      "+----+----+----+----+\n",
      "|2016| 360|null|null|\n",
      "|2017| 312| 360| bad|\n",
      "|2018| 275| 312| bad|\n",
      "|2019| 301| 275|good|\n",
      "|2020| 103| 301| bad|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|year|runs| out|\n",
      "+----+----+----+\n",
      "|2016| 360|good|\n",
      "|2017| 312| bad|\n",
      "|2018| 275| bad|\n",
      "|2019| 301|good|\n",
      "|2020| 103| bad|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_df.withColumn(\"out\",when( col(\"runs\") < lag('runs').over(Window.orderBy(asc('year'))),'bad').otherwise('good')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling None and null in Pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df=spark.createDataFrame([[1,None],[2,\"ravi\"]],('id','name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|null|\n",
      "|  2|ravi|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'null' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-1573a07e866e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#If we type null it will throw error as null is not a value in python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnull_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnull\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ravi\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'null' is not defined"
     ]
    }
   ],
   "source": [
    "#If we type null it will throw error as null is not a value in python\n",
    "null_df=spark.createDataFrame([[1,null],[2,\"ravi\"]],('id','name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df=spark.createDataFrame([[1,None],[2,\"ravi\"],[None,None]],('id','name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|  id|name|\n",
      "+----+----+\n",
      "|   1|null|\n",
      "|   2|ravi|\n",
      "|null|null|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+\n",
      "|  id|name|check_null|\n",
      "+----+----+----------+\n",
      "|   1|null|      true|\n",
      "|   2|ravi|     false|\n",
      "|null|null|      true|\n",
      "+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The isNull function returns True if the value is null and False otherwise.\n",
    "null_df.withColumn(\"check_null\",null_df.name.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df=null_df.withColumn(\"other_id\",lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+----------+\n",
      "|  id|name|other_id|check_null|\n",
      "+----+----+--------+----------+\n",
      "|   1|null|       1|      true|\n",
      "|   2|ravi|       1|     false|\n",
      "|null|null|       1|      null|\n",
      "+----+----+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == equality operator handles comparisons with null values.\n",
    "null_df.withColumn(\"check_null\",null_df.id==null_df.other_id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When one value is null and the other is not null, return False\n",
    "#When both values are null, return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+------------+\n",
      "|  id|name|other_id|num1_eq_num2|\n",
      "+----+----+--------+------------+\n",
      "|   1|null|       1|        true|\n",
      "|   2|ravi|       1|       false|\n",
      "|null|null|       1|       false|\n",
      "+----+----+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df.withColumn(\n",
    "  \"num1_eq_num2\",\n",
    "  when(null_df.id.isNull() & null_df.other_id.isNull(), True)\n",
    "    .when(null_df.id.isNull() | null_df.other_id.isNull(), False)\n",
    "    .otherwise(null_df.id == null_df.other_id)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### null safe equality\n",
    "### We can perform the same null safe equality comparison with the built-in eqNullSafe function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+--------------------+\n",
      "|  id|name|other_id|num1_eqNullSafe_num2|\n",
      "+----+----+--------+--------------------+\n",
      "|   1|null|       1|                true|\n",
      "|   2|ravi|       1|               false|\n",
      "|null|null|       1|               false|\n",
      "+----+----+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_df.withColumn(\"num1_eqNullSafe_num2\", null_df.id.eqNullSafe(null_df.other_id)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
